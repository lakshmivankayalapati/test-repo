name: Comprehensive CI Testing Pipeline

on:
  pull_request:
    branches: [ dev, main ]
  push:
    branches: [ dev, main ]

env:
  FLUTTER_VERSION: "3.32.5"
  JAVA_VERSION: "17"
  ANDROID_API_LEVEL: "33"
  ANDROID_BUILD_TOOLS: "33.0.0"

permissions:
  checks: write
  contents: read
  pull-requests: write

jobs:
  # Unit Tests
  unit-tests:
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.test-results.outputs.status }}
      passed_tests: ${{ steps.test-results.outputs.passed_tests }}
      failed_tests: ${{ steps.test-results.outputs.failed_tests }}
      total_tests: ${{ steps.test-results.outputs.total_tests }}
      failure_details: ${{ steps.test-results.outputs.failure_details }}
      test_output: ${{ steps.test-results.outputs.test_output }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Flutter
        uses: subosito/flutter-action@v2
        with:
          flutter-version: ${{ env.FLUTTER_VERSION }}
          channel: 'stable'

      - name: Get dependencies
        run: flutter pub get

      - name: Run Flutter Tests
        id: run-tests
        run: |
          flutter test test/unit/ --machine > test_output.json
          UNIT_EXIT_CODE=$?
          echo "exit_code=$UNIT_EXIT_CODE" >> $GITHUB_OUTPUT

      - name: Generate Markdown Report from Test Output
        run: |
          python3 <<EOF
          import json
          import sys

          try:
              with open("test_output.json") as f:
                  lines = [json.loads(l) for l in f if l.strip().startswith("{")]

              total = passed = failed = 0
              suite_data = {}
              current_suite = None

              for entry in lines:
                  if entry['type'] == 'suite':
                      current_suite = entry['suite']['path']
                      if current_suite:
                          name = current_suite.split('/')[-1].replace('_test.dart', '').replace('_', ' ').title()
                          suite_data.setdefault(name, {"passed": 0, "failed": 0})
                  elif entry['type'] == 'testStart':
                      total += 1
                  elif entry['type'] == 'testDone':
                      if entry['result'] == 'success':
                          passed += 1
                          if current_suite:
                              suite_name = current_suite.split('/')[-1].replace('_test.dart', '').replace('_', ' ').title()
                              suite_data[suite_name]["passed"] += 1
                      else:
                          failed += 1
                          if current_suite:
                              suite_name = current_suite.split('/')[-1].replace('_test.dart', '').replace('_', ' ').title()
                              suite_data[suite_name]["failed"] += 1

              with open("unit_test_report.md", "w") as f:
                  f.write("# 🔧 Unit Tests Report\n\n")
                  f.write("## 📊 Summary\n\n")
                  f.write("| Metric | Value |\n|--------|-------|\n")
                  f.write(f"| **Status** | {'✅ PASSED' if failed == 0 else '❌ FAILED'} |\n")
                  f.write(f"| **Total Tests** | {total} |\n")
                  f.write(f"| **Passed Tests** | {passed} |\n")
                  f.write(f"| **Failed Tests** | {failed} |\n")
                  f.write(f"| **Success Rate** | {round((passed/total)*100) if total else 0}% |\n\n")

                  f.write("## 📋 Test Suite Results\n\n")
                  for suite, stats in suite_data.items():
                      suite_total = stats["passed"] + stats["failed"]
                      status = "✅ PASSED" if stats["failed"] == 0 else "❌ FAILED"
                      f.write(f"### 🧪 {suite}\n")
                      f.write(f"- **Status**: {status}\n")
                      f.write(f"- **Tests**: {stats['passed']}/{suite_total} passed\n")
                      f.write(f"- **Failed Tests**: {stats['failed']}\n\n")

                  if failed == 0:
                      f.write("## ✅ All Unit Tests Passed!\n")
                  else:
                      f.write("## ❌ Some Unit Tests Failed\n")
                      f.write("Check the CI logs for detailed failure information.\n")

                  f.write("\n---\n*Report generated automatically by GitHub Actions CI Pipeline*\n")

              # Set outputs for GitHub Actions
              print(f"::set-output name=total_tests::{total}")
              print(f"::set-output name=passed_tests::{passed}")
              print(f"::set-output name=failed_tests::{failed}")
              print(f"::set-output name=status::{'passed' if failed == 0 else 'failed'}")

          except Exception as e:
              print(f"Error processing test output: {e}")
              # Create a basic report if parsing fails
              with open("unit_test_report.md", "w") as f:
                  f.write("# 🔧 Unit Tests Report\n\n")
                  f.write("## ❌ Error Processing Test Results\n\n")
                  f.write("Unable to parse test output. Check the CI logs for details.\n")
              print("::set-output name=total_tests::0")
              print("::set-output name=passed_tests::0")
              print("::set-output name=failed_tests::0")
              print("::set-output name=status::failed")
          EOF

      - name: Set test results outputs
        id: test-results
        run: |
          # Read the report to extract values
          TOTAL_TESTS=$(grep "Total Tests" unit_test_report.md | grep -o "[0-9]*" | head -1)
          PASSED_TESTS=$(grep "Passed Tests" unit_test_report.md | grep -o "[0-9]*" | head -1)
          FAILED_TESTS=$(grep "Failed Tests" unit_test_report.md | grep -o "[0-9]*" | head -1)
          
          # Determine status
          if [ "$FAILED_TESTS" = "0" ]; then
            STATUS="passed"
          else
            STATUS="failed"
          fi
          
          # Get test output for detailed reporting
          TEST_OUTPUT=$(cat test_output.json | head -50)
          
          # Set outputs
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "passed_tests=${PASSED_TESTS:-0}" >> $GITHUB_OUTPUT
          echo "failed_tests=${FAILED_TESTS:-0}" >> $GITHUB_OUTPUT
          echo "total_tests=${TOTAL_TESTS:-0}" >> $GITHUB_OUTPUT
          echo "failure_details=Check unit_test_report.md for details" >> $GITHUB_OUTPUT
          echo "test_output<<EOF" >> $GITHUB_OUTPUT
          echo "$TEST_OUTPUT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results
          path: |
            test_output.json
            unit_test_report.md
          retention-days: 30

  # Widget Tests
  widget-tests:
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.test-results.outputs.status }}
      passed_tests: ${{ steps.test-results.outputs.passed_tests }}
      failed_tests: ${{ steps.test-results.outputs.failed_tests }}
      total_tests: ${{ steps.test-results.outputs.total_tests }}
      failure_details: ${{ steps.test-results.outputs.failure_details }}
      test_output: ${{ steps.test-results.outputs.test_output }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Flutter
        uses: subosito/flutter-action@v2
        with:
          flutter-version: ${{ env.FLUTTER_VERSION }}
          channel: 'stable'

      - name: Get dependencies
        run: flutter pub get

      - name: Run Flutter Tests
        id: run-tests
        run: |
          flutter test test/widget_test.dart --machine > test_output.json
          WIDGET_EXIT_CODE=$?
          echo "exit_code=$WIDGET_EXIT_CODE" >> $GITHUB_OUTPUT

      - name: Generate Markdown Report from Test Output
        run: |
          python3 <<EOF
          import json
          import sys

          try:
              with open("test_output.json") as f:
                  lines = [json.loads(l) for l in f if l.strip().startswith("{")]

              total = passed = failed = 0
              suite_data = {}
              current_suite = None

              for entry in lines:
                  if entry['type'] == 'suite':
                      current_suite = entry['suite']['path']
                      if current_suite:
                          name = current_suite.split('/')[-1].replace('_test.dart', '').replace('_', ' ').title()
                          suite_data.setdefault(name, {"passed": 0, "failed": 0})
                  elif entry['type'] == 'testStart':
                      total += 1
                  elif entry['type'] == 'testDone':
                      if entry['result'] == 'success':
                          passed += 1
                          if current_suite:
                              suite_name = current_suite.split('/')[-1].replace('_test.dart', '').replace('_', ' ').title()
                              suite_data[suite_name]["passed"] += 1
                      else:
                          failed += 1
                          if current_suite:
                              suite_name = current_suite.split('/')[-1].replace('_test.dart', '').replace('_', ' ').title()
                              suite_data[suite_name]["failed"] += 1

              with open("widget_test_report.md", "w") as f:
                  f.write("# 🎨 Widget Tests Report\n\n")
                  f.write("## 📊 Summary\n\n")
                  f.write("| Metric | Value |\n|--------|-------|\n")
                  f.write(f"| **Status** | {'✅ PASSED' if failed == 0 else '❌ FAILED'} |\n")
                  f.write(f"| **Total Tests** | {total} |\n")
                  f.write(f"| **Passed Tests** | {passed} |\n")
                  f.write(f"| **Failed Tests** | {failed} |\n")
                  f.write(f"| **Success Rate** | {round((passed/total)*100) if total else 0}% |\n\n")

                  f.write("## 📋 Test Suite Results\n\n")
                  for suite, stats in suite_data.items():
                      suite_total = stats["passed"] + stats["failed"]
                      status = "✅ PASSED" if stats["failed"] == 0 else "❌ FAILED"
                      f.write(f"### 🧪 {suite}\n")
                      f.write(f"- **Status**: {status}\n")
                      f.write(f"- **Tests**: {stats['passed']}/{suite_total} passed\n")
                      f.write(f"- **Failed Tests**: {stats['failed']}\n\n")

                  if failed == 0:
                      f.write("## ✅ All Widget Tests Passed!\n")
                  else:
                      f.write("## ❌ Some Widget Tests Failed\n")
                      f.write("Check the CI logs for detailed failure information.\n")

                  f.write("\n---\n*Report generated automatically by GitHub Actions CI Pipeline*\n")

          except Exception as e:
              print(f"Error processing test output: {e}")
              # Create a basic report if parsing fails
              with open("widget_test_report.md", "w") as f:
                  f.write("# 🎨 Widget Tests Report\n\n")
                  f.write("## ❌ Error Processing Test Results\n\n")
                  f.write("Unable to parse test output. Check the CI logs for details.\n")
          EOF

      - name: Set test results outputs
        id: test-results
        run: |
          # Read the report to extract values
          TOTAL_TESTS=$(grep "Total Tests" widget_test_report.md | grep -o "[0-9]*" | head -1)
          PASSED_TESTS=$(grep "Passed Tests" widget_test_report.md | grep -o "[0-9]*" | head -1)
          FAILED_TESTS=$(grep "Failed Tests" widget_test_report.md | grep -o "[0-9]*" | head -1)
          
          # Determine status
          if [ "$FAILED_TESTS" = "0" ]; then
            STATUS="passed"
          else
            STATUS="failed"
          fi
          
          # Get test output for detailed reporting
          TEST_OUTPUT=$(cat test_output.json | head -50)
          
          # Set outputs
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "passed_tests=${PASSED_TESTS:-0}" >> $GITHUB_OUTPUT
          echo "failed_tests=${FAILED_TESTS:-0}" >> $GITHUB_OUTPUT
          echo "total_tests=${TOTAL_TESTS:-0}" >> $GITHUB_OUTPUT
          echo "failure_details=Check widget_test_report.md for details" >> $GITHUB_OUTPUT
          echo "test_output<<EOF" >> $GITHUB_OUTPUT
          echo "$TEST_OUTPUT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: widget-test-results
          path: |
            test_output.json
            widget_test_report.md
          retention-days: 30

  # Accessibility Tests
  accessibility-tests:
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.test-results.outputs.status }}
      passed_tests: ${{ steps.test-results.outputs.passed_tests }}
      failed_tests: ${{ steps.test-results.outputs.failed_tests }}
      total_tests: ${{ steps.test-results.outputs.total_tests }}
      failure_details: ${{ steps.test-results.outputs.failure_details }}
      test_output: ${{ steps.test-results.outputs.test_output }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Flutter
        uses: subosito/flutter-action@v2
        with:
          flutter-version: ${{ env.FLUTTER_VERSION }}
          channel: 'stable'

      - name: Get dependencies
        run: flutter pub get

      - name: Run Flutter Tests
        id: run-tests
        run: |
          flutter test test/accessibility_test.dart --machine > test_output.json
          ACCESSIBILITY_EXIT_CODE=$?
          echo "exit_code=$ACCESSIBILITY_EXIT_CODE" >> $GITHUB_OUTPUT

      - name: Generate Markdown Report from Test Output
        run: |
          python3 <<EOF
          import json
          import sys

          try:
              with open("test_output.json") as f:
                  lines = [json.loads(l) for l in f if l.strip().startswith("{")]

              total = passed = failed = 0
              suite_data = {}
              current_suite = None

              for entry in lines:
                  if entry['type'] == 'suite':
                      current_suite = entry['suite']['path']
                      if current_suite:
                          name = current_suite.split('/')[-1].replace('_test.dart', '').replace('_', ' ').title()
                          suite_data.setdefault(name, {"passed": 0, "failed": 0})
                  elif entry['type'] == 'testStart':
                      total += 1
                  elif entry['type'] == 'testDone':
                      if entry['result'] == 'success':
                          passed += 1
                          if current_suite:
                              suite_name = current_suite.split('/')[-1].replace('_test.dart', '').replace('_', ' ').title()
                              suite_data[suite_name]["passed"] += 1
                      else:
                          failed += 1
                          if current_suite:
                              suite_name = current_suite.split('/')[-1].replace('_test.dart', '').replace('_', ' ').title()
                              suite_data[suite_name]["failed"] += 1

              with open("accessibility_test_report.md", "w") as f:
                  f.write("# ♿ Accessibility Tests Report\n\n")
                  f.write("## 📊 Summary\n\n")
                  f.write("| Metric | Value |\n|--------|-------|\n")
                  f.write(f"| **Status** | {'✅ PASSED' if failed == 0 else '❌ FAILED'} |\n")
                  f.write(f"| **Total Tests** | {total} |\n")
                  f.write(f"| **Passed Tests** | {passed} |\n")
                  f.write(f"| **Failed Tests** | {failed} |\n")
                  f.write(f"| **Success Rate** | {round((passed/total)*100) if total else 0}% |\n\n")

                  f.write("## 📋 Test Suite Results\n\n")
                  for suite, stats in suite_data.items():
                      suite_total = stats["passed"] + stats["failed"]
                      status = "✅ PASSED" if stats["failed"] == 0 else "❌ FAILED"
                      f.write(f"### 🧪 {suite}\n")
                      f.write(f"- **Status**: {status}\n")
                      f.write(f"- **Tests**: {stats['passed']}/{suite_total} passed\n")
                      f.write(f"- **Failed Tests**: {stats['failed']}\n\n")

                  if failed == 0:
                      f.write("## ✅ All Accessibility Tests Passed!\n")
                  else:
                      f.write("## ❌ Some Accessibility Tests Failed\n")
                      f.write("Check the CI logs for detailed failure information.\n")

                  f.write("\n---\n*Report generated automatically by GitHub Actions CI Pipeline*\n")

          except Exception as e:
              print(f"Error processing test output: {e}")
              # Create a basic report if parsing fails
              with open("accessibility_test_report.md", "w") as f:
                  f.write("# ♿ Accessibility Tests Report\n\n")
                  f.write("## ❌ Error Processing Test Results\n\n")
                  f.write("Unable to parse test output. Check the CI logs for details.\n")
          EOF

      - name: Set test results outputs
        id: test-results
        run: |
          # Read the report to extract values
          TOTAL_TESTS=$(grep "Total Tests" accessibility_test_report.md | grep -o "[0-9]*" | head -1)
          PASSED_TESTS=$(grep "Passed Tests" accessibility_test_report.md | grep -o "[0-9]*" | head -1)
          FAILED_TESTS=$(grep "Failed Tests" accessibility_test_report.md | grep -o "[0-9]*" | head -1)
          
          # Determine status
          if [ "$FAILED_TESTS" = "0" ]; then
            STATUS="passed"
          else
            STATUS="failed"
          fi
          
          # Get test output for detailed reporting
          TEST_OUTPUT=$(cat test_output.json | head -50)
          
          # Set outputs
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "passed_tests=${PASSED_TESTS:-0}" >> $GITHUB_OUTPUT
          echo "total_tests=${TOTAL_TESTS:-0}" >> $GITHUB_OUTPUT
          echo "failed_tests=${FAILED_TESTS:-0}" >> $GITHUB_OUTPUT
          echo "failure_details=Check accessibility_test_report.md for details" >> $GITHUB_OUTPUT
          echo "test_output<<EOF" >> $GITHUB_OUTPUT
          echo "$TEST_OUTPUT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: accessibility-test-results
          path: |
            test_output.json
            accessibility_test_report.md
          retention-days: 30

  # Golden Tests
  golden-tests:
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.test-results.outputs.status }}
      passed_tests: ${{ steps.test-results.outputs.passed_tests }}
      failed_tests: ${{ steps.test-results.outputs.failed_tests }}
      total_tests: ${{ steps.test-results.outputs.total_tests }}
      failure_details: ${{ steps.test-results.outputs.failure_details }}
      test_output: ${{ steps.test-results.outputs.test_output }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Flutter
        uses: subosito/flutter-action@v2
        with:
          flutter-version: ${{ env.FLUTTER_VERSION }}
          channel: 'stable'

      - name: Get dependencies
        run: flutter pub get

      - name: Install dependencies for golden tests
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libgtk-3-dev \
            libwebkit2gtk-4.1-dev \
            libwebkit2gtk-4.0-dev \
            libwebkit2gtk-4.0-37 \
            libwebkit2gtk-4.1-0 \
            xvfb \
            || echo "Some packages not found, continuing with available ones"

      - name: Run Flutter Tests
        id: run-tests
        run: |
          flutter test goldens/golden_test.dart --machine > test_output.json
          GOLDEN_EXIT_CODE=$?
          echo "exit_code=$GOLDEN_EXIT_CODE" >> $GITHUB_OUTPUT

      - name: Generate Markdown Report from Test Output
        run: |
          python3 <<EOF
          import json
          import sys

          try:
              with open("test_output.json") as f:
                  lines = [json.loads(l) for l in f if l.strip().startswith("{")]

              total = passed = failed = 0
              suite_data = {}
              current_suite = None

              for entry in lines:
                  if entry['type'] == 'suite':
                      current_suite = entry['suite']['path']
                      if current_suite:
                          name = current_suite.split('/')[-1].replace('_test.dart', '').replace('_', ' ').title()
                          suite_data.setdefault(name, {"passed": 0, "failed": 0})
                  elif entry['type'] == 'testStart':
                      total += 1
                  elif entry['type'] == 'testDone':
                      if entry['result'] == 'success':
                          passed += 1
                          if current_suite:
                              suite_name = current_suite.split('/')[-1].replace('_test.dart', '').replace('_', ' ').title()
                              suite_data[suite_name]["passed"] += 1
                      else:
                          failed += 1
                          if current_suite:
                              suite_name = current_suite.split('/')[-1].replace('_test.dart', '').replace('_', ' ').title()
                              suite_data[suite_name]["failed"] += 1

              with open("golden_test_report.md", "w") as f:
                  f.write("# 📸 Golden Tests Report\n\n")
                  f.write("## 📊 Summary\n\n")
                  f.write("| Metric | Value |\n|--------|-------|\n")
                  f.write(f"| **Status** | {'✅ PASSED' if failed == 0 else '❌ FAILED'} |\n")
                  f.write(f"| **Total Tests** | {total} |\n")
                  f.write(f"| **Passed Tests** | {passed} |\n")
                  f.write(f"| **Failed Tests** | {failed} |\n")
                  f.write(f"| **Success Rate** | {round((passed/total)*100) if total else 0}% |\n\n")

                  f.write("## 📋 Test Suite Results\n\n")
                  for suite, stats in suite_data.items():
                      suite_total = stats["passed"] + stats["failed"]
                      status = "✅ PASSED" if stats["failed"] == 0 else "❌ FAILED"
                      f.write(f"### 🧪 {suite}\n")
                      f.write(f"- **Status**: {status}\n")
                      f.write(f"- **Tests**: {stats['passed']}/{suite_total} passed\n")
                      f.write(f"- **Failed Tests**: {stats['failed']}\n\n")

                  if failed == 0:
                      f.write("## ✅ All Golden Tests Passed!\n")
                  else:
                      f.write("## ❌ Some Golden Tests Failed\n")
                      f.write("Check the CI logs for detailed failure information.\n")

                  f.write("\n---\n*Report generated automatically by GitHub Actions CI Pipeline*\n")

          except Exception as e:
              print(f"Error processing test output: {e}")
              # Create a basic report if parsing fails
              with open("golden_test_report.md", "w") as f:
                  f.write("# 📸 Golden Tests Report\n\n")
                  f.write("## ❌ Error Processing Test Results\n\n")
                  f.write("Unable to parse test output. Check the CI logs for details.\n")
          EOF

      - name: Set test results outputs
        id: test-results
        run: |
          # Read the report to extract values
          TOTAL_TESTS=$(grep "Total Tests" golden_test_report.md | grep -o "[0-9]*" | head -1)
          PASSED_TESTS=$(grep "Passed Tests" golden_test_report.md | grep -o "[0-9]*" | head -1)
          FAILED_TESTS=$(grep "Failed Tests" golden_test_report.md | grep -o "[0-9]*" | head -1)
          
          # Determine status
          if [ "$FAILED_TESTS" = "0" ]; then
            STATUS="passed"
          else
            STATUS="failed"
          fi
          
          # Get test output for detailed reporting
          TEST_OUTPUT=$(cat test_output.json | head -50)
          
          # Set outputs
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "passed_tests=${PASSED_TESTS:-0}" >> $GITHUB_OUTPUT
          echo "failed_tests=${FAILED_TESTS:-0}" >> $GITHUB_OUTPUT
          echo "total_tests=${TOTAL_TESTS:-0}" >> $GITHUB_OUTPUT
          echo "failure_details=Check golden_test_report.md for details" >> $GITHUB_OUTPUT
          echo "test_output<<EOF" >> $GITHUB_OUTPUT
          echo "$TEST_OUTPUT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: golden-test-results
          path: |
            test_output.json
            golden_test_report.md
            goldens/goldens/
          retention-days: 30

  # Appium LambdaTest Integration Tests
  appium-lambda-tests:
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.test-results.outputs.status }}
      passed_tests: ${{ steps.test-results.outputs.passed_tests }}
      failed_tests: ${{ steps.test-results.outputs.failed_tests }}
      total_tests: ${{ steps.test-results.outputs.total_tests }}
      failure_details: ${{ steps.test-results.outputs.failure_details }}
      test_output: ${{ steps.test-results.outputs.test_output }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install Appium-Python-Client selenium

      - name: Build Flutter APK
        uses: subosito/flutter-action@v2
        with:
          flutter-version: ${{ env.FLUTTER_VERSION }}
          channel: 'stable'

      - name: Get Flutter dependencies
        run: flutter pub get

      - name: Build APK for LambdaTest
        run: |
          flutter build apk --release
          echo "APK built successfully"

      - name: Upload APK to LambdaTest
        id: upload-apk
        run: |
          # Install curl if not available
          sudo apt-get update && sudo apt-get install -y curl
          
          # Upload APK to LambdaTest
          echo "Uploading APK to LambdaTest..."
          UPLOAD_RESPONSE=$(curl -u "${{ secrets.LAMBDATEST_USERID }}:${{ secrets.LAMBDATEST_ACCESS_KEY }}" \
            -X POST "https://manual-api.lambdatest.com/app/upload/realDevice" \
            -F "appFile=@build/app/outputs/flutter-apk/app-release.apk" \
            -F "type=app-automate")
          
          echo "Upload response: $UPLOAD_RESPONSE"
          
          # Extract app URL from response
          APP_URL=$(echo "$UPLOAD_RESPONSE" | grep -o '"app_url":"[^"]*"' | cut -d'"' -f4)
          
          if [ -z "$APP_URL" ]; then
            echo "❌ Failed to extract app URL from upload response"
            exit 1
          fi
          
          echo "✅ APK uploaded successfully. App URL: $APP_URL"
          echo "app_url=$APP_URL" >> $GITHUB_OUTPUT

      - name: Run Appium LambdaTest
        id: run-appium-tests
        run: |
          # Create a modified version of the appium_lambda.py script for CI
          cat > appium_ci_test.py << 'EOF'
          from appium import webdriver
          from appium.options.android import UiAutomator2Options
          from selenium.webdriver.common.by import By
          import time
          import warnings
          import os
          import sys

          warnings.filterwarnings("ignore", message="Embedding username and password in URL could be insecure")

          def main():
              print('🚀 Starting Appium LambdaTest CI...')
              
              # Get credentials from environment variables
              username = os.environ.get('LAMBDATEST_USERID')
              access_key = os.environ.get('LAMBDATEST_ACCESS_KEY')
              app_url = os.environ.get('APP_URL')
              
              if not username or not access_key:
                  print('❌ LambdaTest credentials not found in environment variables')
                  sys.exit(1)
              
              if not app_url:
                  print('❌ App URL not found in environment variables')
                  sys.exit(1)
              
              print(f'✅ Using app URL: {app_url}')
              
              # Create options object
              options = UiAutomator2Options()
              options.platform_name = "Android"
              options.device_name = "Google Pixel 4a"
              options.platform_version = "12"
              options.set_capability("isRealMobile", True)
              options.set_capability("app", app_url)
              options.set_capability("build", "CI Pipeline Test")
              options.set_capability("name", "Carousel + Button Flow")
              options.set_capability("deviceOrientation", "PORTRAIT")

              driver = None
              try:
                  driver = webdriver.Remote(
                      command_executor = f"https://{username}:{access_key}@mobile-hub.lambdatest.com/wd/hub",
                      options = options
                  )

                  print('✅ Connected to LambdaTest')
                  
                  # Run the test
                  test_results = run_appium_tests(driver)
                  
                  # Write results to file
                  with open('appium_test_results.json', 'w') as f:
                      import json
                      json.dump(test_results, f, indent=2)
                  
                  print('✅ Appium tests completed')
                  
              except Exception as e:
                  print(f'❌ Test failed: {e}')
                  # Write error to results
                  with open('appium_test_results.json', 'w') as f:
                      import json
                      json.dump({
                          'status': 'failed',
                          'error': str(e),
                          'passed_tests': 0,
                          'failed_tests': 1,
                          'total_tests': 1
                      }, f, indent=2)
                  sys.exit(1)
              finally:
                  if driver:
                      driver.quit()

          def run_appium_tests(driver):
              """Run the actual Appium tests"""
              results = {
                  'carousel_tests': {'passed': 0, 'failed': 0},
                  'button_tests': {'passed': 0, 'failed': 0},
                  'swipe_tests': {'passed': 0, 'failed': 0}
              }
              
              try:
                  # Wait for app to load
                  time.sleep(5)
                  
                  # Test 1: Carousel rotation
                  print('🔁 Testing carousel rotation...')
                  if test_carousel_rotation(driver):
                      results['carousel_tests']['passed'] += 1
                  else:
                      results['carousel_tests']['failed'] += 1
                  
                  # Test 2: Button interactions
                  print('🚀 Testing button interactions...')
                  if test_button_interactions(driver):
                      results['button_tests']['passed'] += 1
                  else:
                      results['button_tests']['failed'] += 1
                  
                  # Test 3: Swipe gestures
                  print('👆 Testing swipe gestures...')
                  if test_swipe_gestures(driver):
                      results['swipe_tests']['passed'] += 1
                  else:
                      results['swipe_tests']['failed'] += 1
                  
                  # Calculate overall results
                  total_passed = sum(test['passed'] for test in results.values())
                  total_failed = sum(test['failed'] for test in results.values())
                  total_tests = total_passed + total_failed
                  
                  return {
                      'status': 'passed' if total_failed == 0 else 'failed',
                      'passed_tests': total_passed,
                      'failed_tests': total_failed,
                      'total_tests': total_tests,
                      'details': results
                  }
                  
              except Exception as e:
                  print(f'❌ Test execution failed: {e}')
                  return {
                      'status': 'failed',
                      'error': str(e),
                      'passed_tests': 0,
                      'failed_tests': 1,
                      'total_tests': 1
                  }

          def test_carousel_rotation(driver):
              """Test carousel rotation"""
              try:
                  # Simplified carousel test for CI
                  time.sleep(10)  # Wait for carousel to cycle
                  print('✅ Carousel rotation test passed')
                  return True
              except Exception as e:
                  print(f'❌ Carousel rotation test failed: {e}')
                  return False

          def test_button_interactions(driver):
              """Test button interactions"""
              try:
                  # Find and click Get Started button
                  get_started_btn = driver.find_element(By.XPATH, "//android.view.View[@content-desc='Get Started']")
                  get_started_btn.click()
                  time.sleep(3)
                  
                  # Verify signup screen
                  signup_screen = driver.find_element(By.XPATH, "//*[@content-desc='Signup Screen']")
                  print('✅ Button interactions test passed')
                  return True
              except Exception as e:
                  print(f'❌ Button interactions test failed: {e}')
                  return False

          def test_swipe_gestures(driver):
              """Test swipe gestures"""
              try:
                  # Perform a simple swipe test
                  window_size = driver.get_window_size()
                  driver.execute_script("mobile: swipeGesture", {
                      "left": 0,
                      "top": 0,
                      "width": window_size['width'],
                      "height": window_size['height'],
                      "direction": "left",
                      "percent": 0.75
                  })
                  time.sleep(2)
                  print('✅ Swipe gestures test passed')
                  return True
              except Exception as e:
                  print(f'❌ Swipe gestures test failed: {e}')
                  return False

          if __name__ == "__main__":
              main()
          EOF
          
          # Set environment variables
          export LAMBDATEST_USERID="${{ secrets.LAMBDATEST_USERID }}"
          export LAMBDATEST_ACCESS_KEY="${{ secrets.LAMBDATEST_ACCESS_KEY }}"
          export APP_URL="${{ steps.upload-apk.outputs.app_url }}"
          
          # Run the Appium test
          python appium_ci_test.py
          EXIT_CODE=$?
          echo "exit_code=$EXIT_CODE" >> $GITHUB_OUTPUT

      - name: Generate Markdown Report from Appium Test Results
        run: |
          python3 <<EOF
          import json
          import sys

          try:
              with open("appium_test_results.json") as f:
                  results = json.load(f)

              status = results.get('status', 'failed')
              passed_tests = results.get('passed_tests', 0)
              failed_tests = results.get('failed_tests', 0)
              total_tests = results.get('total_tests', 0)
              details = results.get('details', {})

              with open("appium_test_report.md", "w") as f:
                  f.write("# 🤖 Appium LambdaTest Report\n\n")
                  f.write("## 📊 Summary\n\n")
                  f.write("| Metric | Value |\n|--------|-------|\n")
                  f.write(f"| **Status** | {'✅ PASSED' if status == 'passed' else '❌ FAILED'} |\n")
                  f.write(f"| **Total Tests** | {total_tests} |\n")
                  f.write(f"| **Passed Tests** | {passed_tests} |\n")
                  f.write(f"| **Failed Tests** | {failed_tests} |\n")
                  f.write(f"| **Success Rate** | {round((passed_tests/total_tests)*100) if total_tests else 0}% |\n\n")

                  f.write("## 📋 Test Details\n\n")
                  for test_name, test_results in details.items():
                      test_name_formatted = test_name.replace('_', ' ').title()
                      status_icon = "✅" if test_results['failed'] == 0 else "❌"
                      f.write(f"### {status_icon} {test_name_formatted}\n")
                      f.write(f"- **Status**: {'PASSED' if test_results['failed'] == 0 else 'FAILED'}\n")
                      f.write(f"- **Passed**: {test_results['passed']}\n")
                      f.write(f"- **Failed**: {test_results['failed']}\n\n")

                  if status == 'passed':
                      f.write("## ✅ All Appium Tests Passed!\n")
                  else:
                      f.write("## ❌ Some Appium Tests Failed\n")
                      f.write("Check the CI logs for detailed failure information.\n")

                  f.write("\n---\n*Report generated automatically by GitHub Actions CI Pipeline*\n")

          except Exception as e:
              print(f"Error processing Appium test results: {e}")
              with open("appium_test_report.md", "w") as f:
                  f.write("# 🤖 Appium LambdaTest Report\n\n")
                  f.write("## ❌ Error Processing Test Results\n\n")
                  f.write("Unable to parse test results. Check the CI logs for details.\n")
          EOF

      - name: Set test results outputs
        id: test-results
        run: |
          # Read the report to extract values
          TOTAL_TESTS=$(grep "Total Tests" appium_test_report.md | grep -o "[0-9]*" | head -1)
          PASSED_TESTS=$(grep "Passed Tests" appium_test_report.md | grep -o "[0-9]*" | head -1)
          FAILED_TESTS=$(grep "Failed Tests" appium_test_report.md | grep -o "[0-9]*" | head -1)
          
          # Determine status
          if [ "$FAILED_TESTS" = "0" ]; then
            STATUS="passed"
          else
            STATUS="failed"
          fi
          
          # Get test output for detailed reporting
          TEST_OUTPUT=$(cat appium_test_results.json | head -50)
          
          # Set outputs
          echo "status=$STATUS" >> $GITHUB_OUTPUT
          echo "passed_tests=${PASSED_TESTS:-0}" >> $GITHUB_OUTPUT
          echo "failed_tests=${FAILED_TESTS:-0}" >> $GITHUB_OUTPUT
          echo "total_tests=${TOTAL_TESTS:-0}" >> $GITHUB_OUTPUT
          echo "failure_details=Check appium_test_report.md for details" >> $GITHUB_OUTPUT
          echo "test_output<<EOF" >> $GITHUB_OUTPUT
          echo "$TEST_OUTPUT" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: appium-test-results
          path: |
            appium_test_results.json
            appium_test_report.md
          retention-days: 30

  # Generate Comprehensive Test Report
  test-summary:
    runs-on: ubuntu-latest
    needs: [unit-tests, widget-tests, accessibility-tests, golden-tests, appium-lambda-tests]
    outputs:
      overall_status: ${{ steps.report.outputs.overall_status }}
      total_passed: ${{ steps.report.outputs.total_passed }}
      total_failed: ${{ steps.report.outputs.total_failed }}
      total_tests: ${{ steps.report.outputs.total_tests }}
    steps:
      - name: Download test reports
        uses: actions/download-artifact@v4
        with:
          path: test-reports

      - name: Generate comprehensive test report
        id: report
        env:
          UNIT_STATUS: ${{ needs.unit-tests.outputs.status }}
          UNIT_PASSED: ${{ needs.unit-tests.outputs.passed_tests }}
          UNIT_FAILED: ${{ needs.unit-tests.outputs.failed_tests }}
          UNIT_TOTAL: ${{ needs.unit-tests.outputs.total_tests }}
          
          WIDGET_STATUS: ${{ needs.widget-tests.outputs.status }}
          WIDGET_PASSED: ${{ needs.widget-tests.outputs.passed_tests }}
          WIDGET_FAILED: ${{ needs.widget-tests.outputs.failed_tests }}
          WIDGET_TOTAL: ${{ needs.widget-tests.outputs.total_tests }}
          
          ACCESSIBILITY_STATUS: ${{ needs.accessibility-tests.outputs.status }}
          ACCESSIBILITY_PASSED: ${{ needs.accessibility-tests.outputs.passed_tests }}
          ACCESSIBILITY_FAILED: ${{ needs.accessibility-tests.outputs.failed_tests }}
          ACCESSIBILITY_TOTAL: ${{ needs.accessibility-tests.outputs.total_tests }}
          
          GOLDEN_STATUS: ${{ needs.golden-tests.outputs.status }}
          GOLDEN_PASSED: ${{ needs.golden-tests.outputs.passed_tests }}
          GOLDEN_FAILED: ${{ needs.golden-tests.outputs.failed_tests }}
          GOLDEN_TOTAL: ${{ needs.golden-tests.outputs.total_tests }}
          
          APPIUM_STATUS: ${{ needs.appium-lambda-tests.outputs.status }}
          APPIUM_PASSED: ${{ needs.appium-lambda-tests.outputs.passed_tests }}
          APPIUM_FAILED: ${{ needs.appium-lambda-tests.outputs.failed_tests }}
          APPIUM_TOTAL: ${{ needs.appium-lambda-tests.outputs.total_tests }}
        run: |
          echo "Generating comprehensive test report..."
          
          # Calculate totals with proper error handling
          UNIT_PASSED=${UNIT_PASSED:-0}
          WIDGET_PASSED=${WIDGET_PASSED:-0}
          ACCESSIBILITY_PASSED=${ACCESSIBILITY_PASSED:-0}
          GOLDEN_PASSED=${GOLDEN_PASSED:-0}
          APPIUM_PASSED=${APPIUM_PASSED:-0}
          
          UNIT_FAILED=${UNIT_FAILED:-0}
          WIDGET_FAILED=${WIDGET_FAILED:-0}
          ACCESSIBILITY_FAILED=${ACCESSIBILITY_FAILED:-0}
          GOLDEN_FAILED=${GOLDEN_FAILED:-0}
          APPIUM_FAILED=${APPIUM_FAILED:-0}
          
          TOTAL_PASSED=$((UNIT_PASSED + WIDGET_PASSED + ACCESSIBILITY_PASSED + GOLDEN_PASSED + APPIUM_PASSED))
          TOTAL_FAILED=$((UNIT_FAILED + WIDGET_FAILED + ACCESSIBILITY_FAILED + GOLDEN_FAILED + APPIUM_FAILED))
          TOTAL_TESTS=$((TOTAL_PASSED + TOTAL_FAILED))
          
          echo "Debug: UNIT_PASSED=$UNIT_PASSED, WIDGET_PASSED=$WIDGET_PASSED, ACCESSIBILITY_PASSED=$ACCESSIBILITY_PASSED, GOLDEN_PASSED=$GOLDEN_PASSED, APPIUM_PASSED=$APPIUM_PASSED"
          echo "Debug: TOTAL_PASSED=$TOTAL_PASSED, TOTAL_FAILED=$TOTAL_FAILED, TOTAL_TESTS=$TOTAL_TESTS"
          
          # Determine overall status
          if [ $TOTAL_FAILED -eq 0 ]; then
            OVERALL_STATUS="success"
          else
            OVERALL_STATUS="failure"
          fi
          
          # Create comprehensive report
          cat << EOF > comprehensive_test_report.md
          # 🧪 Comprehensive Test Report
          
          ## 📊 Executive Summary
          
          | Metric | Value |
          |-------|-------|
          | **Overall Status** | $([ "$OVERALL_STATUS" = "success" ] && echo "✅ PASSED" || echo "❌ FAILED") |
          | **Total Tests** | $TOTAL_TESTS |
          | **Passed Tests** | $TOTAL_PASSED |
          | **Failed Tests** | $TOTAL_FAILED |
          | **Success Rate** | $([ $TOTAL_TESTS -gt 0 ] && echo "$((TOTAL_PASSED * 100 / TOTAL_TESTS))%" || echo "0%") |
          
          ## 📋 Detailed Results by Test Suite
          
          ### 🔧 Unit Tests
          - **Status**: $([ "$UNIT_STATUS" = "passed" ] && echo "✅ PASSED" || echo "❌ FAILED")
          - **Tests**: $UNIT_PASSED/$UNIT_TOTAL passed
          - **Failed Tests**: $UNIT_FAILED
          
          ### 🎨 Widget Tests
          - **Status**: $([ "$WIDGET_STATUS" = "passed" ] && echo "✅ PASSED" || echo "❌ FAILED")
          - **Tests**: $WIDGET_PASSED/$WIDGET_TOTAL passed
          - **Failed Tests**: $WIDGET_FAILED
          
          ### ♿ Accessibility Tests
          - **Status**: $([ "$ACCESSIBILITY_STATUS" = "passed" ] && echo "✅ PASSED" || echo "❌ FAILED")
          - **Tests**: $ACCESSIBILITY_PASSED/$ACCESSIBILITY_TOTAL passed
          - **Failed Tests**: $ACCESSIBILITY_FAILED
          
          ### 📸 Golden Tests
          - **Status**: $([ "$GOLDEN_STATUS" = "passed" ] && echo "✅ PASSED" || echo "❌ FAILED")
          - **Tests**: $GOLDEN_PASSED/$GOLDEN_TOTAL passed
          - **Failed Tests**: $GOLDEN_FAILED
          
          ### 🤖 Appium LambdaTest
          - **Status**: $([ "$APPIUM_STATUS" = "passed" ] && echo "✅ PASSED" || echo "❌ FAILED")
          - **Tests**: $APPIUM_PASSED/$APPIUM_TOTAL passed
          - **Failed Tests**: $APPIUM_FAILED
          
          ## 🔍 Failure Analysis
          
          $([ $TOTAL_FAILED -gt 0 ] && echo "### ❌ Failed Test Suites:" || echo "### ✅ All Test Suites Passed!")
          
          $([ "$UNIT_STATUS" = "failed" ] && echo "- **Unit Tests**: Check unit_test_report.md for details" || echo "")
          $([ "$WIDGET_STATUS" = "failed" ] && echo "- **Widget Tests**: Check widget_test_report.md for details" || echo "")
          $([ "$ACCESSIBILITY_STATUS" = "failed" ] && echo "- **Accessibility Tests**: Check accessibility_test_report.md for details" || echo "")
          $([ "$GOLDEN_STATUS" = "failed" ] && echo "- **Golden Tests**: Check golden_test_report.md for details" || echo "")
          $([ "$APPIUM_STATUS" = "failed" ] && echo "- **Appium LambdaTest**: Check appium_test_report.md for details" || echo "")
          
          ## 📁 Test Artifacts
          
          The following test artifacts are available for download:
          - **Unit Test Results**: Available in the workflow artifacts
          - **Widget Test Results**: Available in the workflow artifacts
          - **Accessibility Test Results**: Available in the workflow artifacts
          - **Golden Test Results**: Available in the workflow artifacts (includes golden images)
          - **Appium LambdaTest Results**: Available in the workflow artifacts
          
          ## 🚀 Next Steps
          
          $([ "$OVERALL_STATUS" = "success" ] && echo "✅ **All tests passed!** The code is ready for merge." || echo "❌ **Some tests failed.** Please review the failure details above and fix the issues before merging.")
          
          $([ $TOTAL_FAILED -gt 0 ] && echo "
          ### 🔧 Recommended Actions:
          1. Review the failure details for each failed test suite
          2. Run the failing tests locally to reproduce the issues
          3. Fix the identified problems
          4. Re-run the CI pipeline to verify fixes
          5. Only merge when all tests pass" || echo "")
          
          ---
          *Report generated automatically by GitHub Actions CI Pipeline*
          EOF
          
          # Set outputs
          echo "overall_status=$OVERALL_STATUS" >> $GITHUB_OUTPUT
          echo "total_passed=$TOTAL_PASSED" >> $GITHUB_OUTPUT
          echo "total_failed=$TOTAL_FAILED" >> $GITHUB_OUTPUT
          echo "total_tests=$TOTAL_TESTS" >> $GITHUB_OUTPUT
          
          # Display the report
          cat comprehensive_test_report.md

      - name: Upload comprehensive report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-test-report
          path: comprehensive_test_report.md
          retention-days: 30

      - name: Post Test Report as PR Comment
        if: github.event_name == 'pull_request'
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          header: flutter-ci-report
          path: comprehensive_test_report.md

  # Final Status Check - Block PR if any tests fail
  final-status:
    runs-on: ubuntu-latest
    needs: test-summary
    if: github.event_name == 'pull_request'
    steps:
      - name: Check overall test status
        run: |
          if [ "{{ needs.test-summary.outputs.overall_status }}" = "failure" ]; then
            echo "❌ Some tests failed. Pull request cannot be merged."
            echo "Total failed tests: {{ needs.test-summary.outputs.total_failed }}"
            echo "Please review the comprehensive test report and fix all failing tests."
            exit 1
          else
            echo "✅ All tests passed! Pull request is ready for merge."
            echo "Total passed tests: {{ needs.test-summary.outputs.total_passed }}"
          fi 