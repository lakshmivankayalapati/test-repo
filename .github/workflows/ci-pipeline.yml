name: Comprehensive CI Testing Pipeline

on:
  pull_request:
    branches: [ dev ]
  push:
    branches: [ dev, feature ]

env:
  FLUTTER_VERSION: "3.32.5"
  JAVA_VERSION: "17"
  ANDROID_API_LEVEL: "33"
  ANDROID_BUILD_TOOLS: "33.0.0"

permissions:
  checks: write
  contents: read
  pull-requests: write

jobs:
  # ---- UNIT TESTS ----
  unit-tests:
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.test-results.outputs.status }}
      passed_tests: ${{ steps.test-results.outputs.passed_tests }}
      failed_tests: ${{ steps.test-results.outputs.failed_tests }}
      total_tests: ${{ steps.test-results.outputs.total_tests }}
      failure_details: ${{ steps.test-results.outputs.failure_details }}
      test_output: ${{ steps.test-results.outputs.test_output }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Flutter
        uses: subosito/flutter-action@v2
        with:
          flutter-version: ${{ env.FLUTTER_VERSION }}
          channel: 'stable'

      - name: Get dependencies
        run: flutter pub get

      - name: Run Flutter Tests
        run: flutter test test/unit/ --machine > test_output.json

      - name: Generate Detailed Markdown Report
        id: test-results
        run: |
          pip install jq
          python3 scripts/parse_flutter_test_output.py test_output.json unit_test_report.md

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results
          path: |
            test_output.json
            unit_test_report.md
          retention-days: 30

  # ---- WIDGET TESTS ----
  widget-tests:
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.test-results.outputs.status }}
      passed_tests: ${{ steps.test-results.outputs.passed_tests }}
      failed_tests: ${{ steps.test-results.outputs.failed_tests }}
      total_tests: ${{ steps.test-results.outputs.total_tests }}
      failure_details: ${{ steps.test-results.outputs.failure_details }}
      test_output: ${{ steps.test-results.outputs.test_output }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Flutter
        uses: subosito/flutter-action@v2
        with:
          flutter-version: ${{ env.FLUTTER_VERSION }}
          channel: 'stable'

      - name: Get dependencies
        run: flutter pub get

      - name: Run Flutter Widget Tests
        run: flutter test test/widget_test.dart --machine > test_output.json

      - name: Generate Detailed Markdown Report
        id: test-results
        run: |
          python3 scripts/parse_flutter_test_output.py test_output.json widget_test_report.md

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: widget-test-results
          path: |
            test_output.json
            widget_test_report.md
          retention-days: 30

  # ---- ACCESSIBILITY TESTS ----
  accessibility-tests:
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.test-results.outputs.status }}
      passed_tests: ${{ steps.test-results.outputs.passed_tests }}
      failed_tests: ${{ steps.test-results.outputs.failed_tests }}
      total_tests: ${{ steps.test-results.outputs.total_tests }}
      failure_details: ${{ steps.test-results.outputs.failure_details }}
      test_output: ${{ steps.test-results.outputs.test_output }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Flutter
        uses: subosito/flutter-action@v2
        with:
          flutter-version: ${{ env.FLUTTER_VERSION }}
          channel: 'stable'

      - name: Get dependencies
        run: flutter pub get

      - name: Run Flutter Accessibility Tests
        run: flutter test test/accessibility_test.dart --machine > test_output.json

      - name: Generate Detailed Markdown Report
        id: test-results
        run: |
          python3 scripts/parse_flutter_test_output.py test_output.json accessibility_test_report.md

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: accessibility-test-results
          path: |
            test_output.json
            accessibility_test_report.md
          retention-days: 30

  # ---- GOLDEN TESTS ----
  golden-tests:
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.test-results.outputs.status }}
      passed_tests: ${{ steps.test-results.outputs.passed_tests }}
      failed_tests: ${{ steps.test-results.outputs.failed_tests }}
      total_tests: ${{ steps.test-results.outputs.total_tests }}
      failure_details: ${{ steps.test-results.outputs.failure_details }}
      test_output: ${{ steps.test-results.outputs.test_output }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Flutter
        uses: subosito/flutter-action@v2
        with:
          flutter-version: ${{ env.FLUTTER_VERSION }}
          channel: 'stable'

      - name: Get dependencies
        run: flutter pub get

      - name: Install dependencies for golden tests
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libgtk-3-dev \
            libwebkit2gtk-4.1-dev \
            libwebkit2gtk-4.0-dev \
            libwebkit2gtk-4.0-37 \
            libwebkit2gtk-4.1-0 \
            xvfb || echo "Some packages not found, continuing..."

      - name: Run Flutter Golden Tests
        run: flutter test goldens/golden_test.dart --machine > test_output.json

      - name: Generate Detailed Markdown Report
        id: test-results
        run: |
          python3 scripts/parse_flutter_test_output.py test_output.json golden_test_report.md

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: golden-test-results
          path: |
            test_output.json
            golden_test_report.md
            goldens/goldens/
          retention-days: 30

  # ---- APPIUM LAMBDATEST INTEGRATION TESTS ----
  appium-lambda-tests:
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.test-results.outputs.status }}
      passed_tests: ${{ steps.test-results.outputs.passed_tests }}
      failed_tests: ${{ steps.test-results.outputs.failed_tests }}
      total_tests: ${{ steps.test-results.outputs.total_tests }}
      failure_details: ${{ steps.test-results.outputs.failure_details }}
      test_output: ${{ steps.test-results.outputs.test_output }}
      app_url: ${{ steps.upload-apk.outputs.app_url }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install Appium-Python-Client selenium

      - name: Build Flutter APK
        uses: subosito/flutter-action@v2
        with:
          flutter-version: ${{ env.FLUTTER_VERSION }}
          channel: 'stable'

      - name: Get Flutter dependencies
        run: flutter pub get

      - name: Build APK for LambdaTest
        run: flutter build apk --release

      - name: Upload APK to LambdaTest
        id: upload-apk
        run: |
          # <Your upload APK script here>
          # Should output app_url to $GITHUB_OUTPUT

      - name: Run Appium LambdaTest Integration Tests
        run: |
          python3 appium_ci_test.py

      - name: Generate Markdown Report from Appium Test Results
        id: test-results
        run: |
          python3 scripts/parse_appium_test_output.py appium_test_results.json appium_test_report.md

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: appium-test-results
          path: |
            appium_test_results.json
            appium_test_report.md
          retention-days: 30

  # ---- PERFORMANCE METRICS TESTS ----
  performance-metrics-tests:
    runs-on: ubuntu-latest
    needs: [appium-lambda-tests]
    outputs:
      status: ${{ steps.test-results.outputs.status }}
      passed_tests: ${{ steps.test-results.outputs.passed_tests }}
      failed_tests: ${{ steps.test-results.outputs.failed_tests }}
      total_tests: ${{ steps.test-results.outputs.total_tests }}
      failure_details: ${{ steps.test-results.outputs.failure_details }}
      test_output: ${{ steps.test-results.outputs.test_output }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install Appium-Python-Client selenium requests statistics jq

      - name: Wait for LambdaTest App Processing
        run: sleep 60

      - name: Run Performance Metrics Test
        id: run-performance-tests
        env:
          LAMBDATEST_USERID: ${{ secrets.LAMBDATEST_USERID }}
          LAMBDATEST_ACCESS_KEY: ${{ secrets.LAMBDATEST_ACCESS_KEY }}
          APP_URL: ${{ needs.appium-lambda-tests.outputs.app_url }}
        run: |
          python3 perf_metrics_ci.py

      - name: Generate Markdown Performance Report
        id: test-results
        run: |
          python3 scripts/parse_perf_test_output.py performance_test_results.json performance_test_report.md

      - name: Upload Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-test-results
          path: |
            performance_test_results.json
            performance_test_report.md
            perf_metrics_ci.py
          retention-days: 30

  # ---- COMPREHENSIVE SUMMARY REPORT ----
  test-summary:
    runs-on: ubuntu-latest
    needs:
      - unit-tests
      - widget-tests
      - accessibility-tests
      - golden-tests
      - appium-lambda-tests
      - performance-metrics-tests
    if: always()
    outputs:
      overall_status: ${{ steps.report.outputs.overall_status }}
      total_passed: ${{ steps.report.outputs.total_passed }}
      total_failed: ${{ steps.report.outputs.total_failed }}
      total_tests: ${{ steps.report.outputs.total_tests }}
    steps:
      - name: Download test reports
        uses: actions/download-artifact@v4
        with:
          path: test-reports

      - name: Generate Comprehensive Test Report
        id: report
        run: |
          python3 scripts/generate_comprehensive_report.py test-reports/ comprehensive_test_report.md

      - name: Upload comprehensive report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-test-report
          path: comprehensive_test_report.md
          retention-days: 30

      - name: Post Test Report as PR Comment
        if: github.event_name == 'pull_request'
        uses: marocchino/sticky-pull-request-comment@v2
        with:
          header: flutter-ci-report
          path: comprehensive_test_report.md

  # ---- FINAL STATUS CHECK ----
  final-status:
    runs-on: ubuntu-latest
    needs:
      - test-summary
      - unit-tests
      - widget-tests
      - accessibility-tests
      - golden-tests
      - appium-lambda-tests
      - performance-metrics-tests
    if: always() && github.event_name == 'pull_request'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check Overall Status
        run: |
          echo "Checking overall test status..."
          # This step would check if any tests failed and block the PR if needed
          echo "âœ… All checks completed"